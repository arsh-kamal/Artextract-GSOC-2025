# ArtExtract GSoC 2025 - Task 1: Convolutional-Recurrent Architectures

## Project Overview
This repository contains my submission for Task 1 of the Google Summer of Code (GSoC) 2025 evaluation test for the "Painting in a Painting" project under the ArtExtract initiative, hosted by the HumanAI Umbrella Organization and the University of Alabama. The task focuses on building a convolutional-recurrent neural network (CNN-RNN) to classify paintings by artist using the WikiArt dataset, as specified in the project guidelines.

**Idea Description:**
- The goal is to leverage a hybrid CNN-RNN model to extract spatial features (via CNN) and sequential patterns (via RNN) from painting images, enabling artist classification. This approach combines ResNet18 for feature extraction with a GRU (Gated Recurrent Unit) to process features sequentially, offering a robust method for art analysis. The model is trained and evaluated on a subset of the WikiArt dataset, with metrics like accuracy, precision, recall, F1-score, and a confusion matrix, plus outlier detection for paintings with low-confidence predictions.

**Mentors:** Emanuele Usai, Sergei Gleyzer (University of Alabama)  
**Submission Date:** March 2025  
**Submitted by:** Arshdeep Kamal

---

## Repository Structure
This repository includes the following files and directories:

### Root Directory
- **`.gitignore`**  
  - **Purpose:** Excludes large or unnecessary files from version control.
  - **Contents:** Ignores the `WikiArt/wikiart/` directory containing the raw image dataset to keep the repository lightweight. The dataset is sourced externally (see below).

### `Artextract-GSOC25/` Directory
- **`artextract_task1.ipynb`**  
  - **Purpose:** Main Jupyter notebook implementing Task 1.
  - **Contents:**
    - Loads `labels.csv` (preprocessed dataset metadata).
    - Defines a custom `WikiArtDataset` class for PyTorch data loading.
    - Implements a `ConvRecurrentModel` combining ResNet18 (CNN) and GRU (RNN).
    - Trains the model on the WikiArt dataset (80% train, 20% test split).
    - Evaluates performance with classification metrics (accuracy, precision, recall, F1-score, confusion matrix).
    - Detects outliers (correct predictions with confidence < 0.5).
    - Saves the trained model as `conv_recurrent_model.pth` (not included due to size).
  - **Execution:** Run in Jupyter Notebook with dependencies (`torch`, `torchvision`, `pandas`, etc.) installed.

- **`artextract_task1.pdf`**  
  - **Purpose:** PDF version of `artextract_task1.ipynb` with executed outputs.
  - **Contents:** Includes code, dataset previews (e.g., "First few rows of labels.csv:"), training logs (e.g., epoch losses), and evaluation results (metrics and outliers). Generated using `wkhtmltopdf` from the HTML export of the notebook.


- **`prepare_dataset.py`**  
  - **Purpose:** Python script to preprocess the WikiArt dataset.
  - **Contents:**
    - Reads `artist_train.csv` from the WikiArt dataset source.
    - Parses file paths into style, artist, and image names.
    - Validates image paths against the `wikiart/` directory (excluded from repo).
    - Outputs `labels.csv` with 10,676 valid image paths and artist labels.
  - **Execution:** Run with `python prepare_dataset.py` in the `WikiArt/` directory with the dataset present.
  - 
### `WikiArt/` Directory
- **`labels.csv`**  
  - **Purpose:** Preprocessed dataset metadata generated by `prepare_dataset.py`.
  - **Contents:** Contains 10,676 rows, each with an `image_path` (e.g., `/Users/Dell/Documents/TASK1/WikiArt/wikiart/Realism/vincent-van-gogh_pine-trees-in-the-fen-1884.jpg`) and `label` (artist ID, e.g., 22 for Vincent van Gogh). Used by `artextract_task1.ipynb` for training and evaluation.

---

## Dataset
- **Source:** The WikiArt dataset is sourced from [ArtGAN WikiArt Dataset](https://github.com/cs-chan/ArtGAN/blob/master/WikiArt%20Dataset/README.md).
- **Processing:** Due to its large size (10,676+ images), the raw `wikiart/` folder is excluded from this repository. Instead, `prepare_dataset.py` processes `artist_train.csv` locally to generate `labels.csv`, which is included here.
- **Note:** To replicate, download the dataset and place it in `WikiArt/wikiart/` before running `prepare_dataset.py`.

---

## How to Run
1. **Install Dependencies**:  
   ```bash
   pip install torch torchvision pandas numpy scikit-learn jupyter
